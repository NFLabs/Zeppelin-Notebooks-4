{
  "paragraphs": [
    {
      "text": "%md \n## Statistical and Mathematical Functions with DataFrames in Spark - Python\n\nFrom [original blog post - Statistical and Mathematical Functions with DataFrames in Spark][1]  \n\nPython, read [pyspark doc][2]   \n1. Random data generation\n2. Summary and descriptive statistics\n3. Sample covariance and correlation\n4. Cross tabulation (a.k.a. contingency table)\n5. Frequent items\n6. Mathematical functions\n\n[1]:https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html\n[2]:https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html\n",
      "dateUpdated": "Dec 8, 2015 5:21:29 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": false,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436860288049_-1876409950",
      "id": "20150714-005128_1583491488",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eStatistical and Mathematical Functions with DataFrames in Spark - Python\u003c/h2\u003e\n\u003cp\u003eFrom \u003ca href\u003d\"https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html\"\u003eoriginal blog post - Statistical and Mathematical Functions with DataFrames in Spark\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePython, read \u003ca href\u003d\"https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html\"\u003epyspark doc\u003c/a\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRandom data generation\u003c/li\u003e\n\u003cli\u003eSummary and descriptive statistics\u003c/li\u003e\n\u003cli\u003eSample covariance and correlation\u003c/li\u003e\n\u003cli\u003eCross tabulation (a.k.a. contingency table)\u003c/li\u003e\n\u003cli\u003eFrequent items\u003c/li\u003e\n\u003cli\u003eMathematical functions\u003c/li\u003e\n\u003c/ol\u003e\n"
      },
      "dateCreated": "Jul 14, 2015 12:51:28 AM",
      "dateStarted": "Dec 8, 2015 5:21:29 PM",
      "dateFinished": "Dec 8, 2015 5:21:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 1. Random Data Generation\nFor \n* testing of existing algorithms \n* implementing randomized algorithms",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436861413972_182788065",
      "id": "20150714-011013_1573413532",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003e1. Random Data Generation\u003c/h3\u003e\n\u003cp\u003eFor\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etesting of existing algorithms\u003c/li\u003e\n\u003cli\u003eimplementing randomized algorithms\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jul 14, 2015 1:10:13 AM",
      "dateStarted": "Jul 14, 2015 1:31:08 AM",
      "dateFinished": "Jul 14, 2015 1:31:08 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import rand, randn\n\n# Need to create a sqlConext, sc is default\nfrom pyspark.sql import SQLContext\nsqlContext \u003d SQLContext(sc)\n\n# Create a DataFrame with one int column and 10 rows.\ndf \u003d sqlContext.range(0, 10)\nprint(type(df))\ndf.show()\n\n# Generate two other columns using\n# uniform distribution \u003d rand(seed\u003d10).alias(\"uniform\")\n# normal distribution \u003d randn(seed\u003d27).alias(\"normal\")\n\ndf.select(\"id\", rand(seed\u003d10).alias(\"uniform\"), randn(seed\u003d27).alias(\"normal\")).show()",
      "dateUpdated": "Dec 6, 2015 3:39:13 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436863394410_-1090679364",
      "id": "20150714-014314_1556677551",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003cclass \u0027pyspark.sql.dataframe.DataFrame\u0027\u003e\n+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n+---+\n\n+---+-------------------+-------------------+\n| id|            uniform|             normal|\n+---+-------------------+-------------------+\n|  0| 0.7224977951905031|-0.1875348803463305|\n|  1| 0.3312021111290707|-0.8692578137288678|\n|  2| 0.2438486392697814| -2.372340011831022|\n|  3|   0.48751036821683| -1.245588815004436|\n|  4| 0.6684543991585701|-0.6032161065605888|\n|  5|0.24378101415107944| -0.575915283914948|\n|  6|   0.31725730968555| 0.5023738038011114|\n|  7| 0.2009218252543502|0.44777575307472967|\n|  8| 0.5173637377779307|  0.600053806707523|\n|  9| 0.9528301401955117|-0.7324006767271523|\n+---+-------------------+-------------------+\n\n"
      },
      "dateCreated": "Jul 14, 2015 1:43:14 AM",
      "dateStarted": "Dec 6, 2015 3:39:13 AM",
      "dateFinished": "Dec 6, 2015 3:39:13 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 2. Summary and descriptive statistics\n",
      "dateUpdated": "Dec 7, 2015 7:30:24 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436861405820_-975706197",
      "id": "20150714-011005_124751323",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003e2. Summary and descriptive statistics\u003c/h3\u003e\n"
      },
      "dateCreated": "Jul 14, 2015 1:10:05 AM",
      "dateStarted": "Jul 14, 2015 4:28:04 AM",
      "dateFinished": "Jul 14, 2015 4:28:04 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import rand, randn\n\n# A slightly different way to generate the two random columns\ndf \u003d sqlContext.range(0, 10).withColumn(\u0027uniform\u0027, rand(seed\u003d10)).withColumn(\u0027normal\u0027, randn(seed\u003d27))\n\ndf.describe().show()\n\n# When there are many columns in a DF, we can just what select the columns we want with `describe` function.\ndf.describe(\u0027uniform\u0027, \u0027normal\u0027).show()\n\n# We can also control the list of descriptive statistics and the columns they apply to using the normal select on a DataFrame:\nfrom pyspark.sql.functions import mean, min, max\n\n# For the same column\ndf.select([mean(\u0027uniform\u0027), min(\u0027uniform\u0027), max(\u0027uniform\u0027)]).show()\n\n# For different columns\ndf.select([mean(\u0027uniform\u0027), mean(\u0027normal\u0027)]).show()\n",
      "dateUpdated": "Dec 7, 2015 6:39:25 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436942026610_1576711445",
      "id": "20150714-233346_1705704236",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+------------------+-------------------+-------------------+\n|summary|                id|            uniform|             normal|\n+-------+------------------+-------------------+-------------------+\n|  count|                10|                 10|                 10|\n|   mean|               4.5|0.46856673400291776|-0.5036050224529982|\n| stddev|2.8722813232690143| 0.2358201303075058| 0.8648117073061448|\n|    min|                 0| 0.2009218252543502| -2.372340011831022|\n|    max|                 9| 0.9528301401955117|  0.600053806707523|\n+-------+------------------+-------------------+-------------------+\n\n+-------+-------------------+-------------------+\n|summary|            uniform|             normal|\n+-------+-------------------+-------------------+\n|  count|                 10|                 10|\n|   mean|0.46856673400291776|-0.5036050224529982|\n| stddev| 0.2358201303075058| 0.8648117073061448|\n|    min| 0.2009218252543502| -2.372340011831022|\n|    max| 0.9528301401955117|  0.600053806707523|\n+-------+-------------------+-------------------+\n\n+-------------------+------------------+------------------+\n|       avg(uniform)|      min(uniform)|      max(uniform)|\n+-------------------+------------------+------------------+\n|0.46856673400291776|0.2009218252543502|0.9528301401955117|\n+-------------------+------------------+------------------+\n\n+-------------------+-------------------+\n|       avg(uniform)|        avg(normal)|\n+-------------------+-------------------+\n|0.46856673400291776|-0.5036050224529982|\n+-------------------+-------------------+\n\n"
      },
      "dateCreated": "Jul 14, 2015 11:33:46 PM",
      "dateStarted": "Dec 7, 2015 6:39:25 AM",
      "dateFinished": "Dec 7, 2015 6:39:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n### 3. Covariance and correlation\n\nCovariance is a measure of how two variables change with respect to each other. \n\n* A positive number would mean that there is a tendency that as one variable increases, the other increases as well. \n* A negative number would mean that as one variable increases, the other variable has a tendency to decrease.\n\nCorrelation is a normalized measure of covariance.",
      "dateUpdated": "Dec 7, 2015 8:51:49 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436861402194_1564036617",
      "id": "20150714-011002_1888718019",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003e3. Covariance and correlation\u003c/h3\u003e\n\u003cp\u003eCovariance is a measure of how two variables change with respect to each other.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA positive number would mean that there is a tendency that as one variable increases, the other increases as well.\u003c/li\u003e\n\u003cli\u003eA negative number would mean that as one variable increases, the other variable has a tendency to decrease.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jul 14, 2015 1:10:02 AM",
      "dateStarted": "Dec 7, 2015 7:29:28 AM",
      "dateFinished": "Dec 7, 2015 7:29:28 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import rand\ndf \u003d sqlContext.range(0, 10).withColumn(\u0027rand1\u0027, rand(seed\u003d10)).withColumn(\u0027rand2\u0027, rand(seed\u003d27))\ndf.show()\n\n# Covariance\ndf.stat.cov(\u0027rand1\u0027, \u0027rand2\u0027)   # why no output?\nprint(df.stat.cov(\u0027rand1\u0027, \u0027rand2\u0027))\n\nprint(df.stat.cov(\u0027id\u0027, \u0027id\u0027))\n\n# Correlation  \ndf.stat.corr(\u0027rand1\u0027, \u0027rand2\u0027)  # why no output?\nprint(df.stat.corr(\u0027rand1\u0027, \u0027rand2\u0027))\n\nprint(df.stat.corr(\u0027id\u0027, \u0027id\u0027))\n",
      "dateUpdated": "Dec 7, 2015 6:28:38 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449473383269_1440934286",
      "id": "20151207-072943_655942418",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-------------------+-------------------+\n| id|              rand1|              rand2|\n+---+-------------------+-------------------+\n|  0| 0.7224977951905031|0.41346994949717164|\n|  1| 0.3312021111290707|  0.098917692628539|\n|  2| 0.2438486392697814|0.42644755590423766|\n|  3|   0.48751036821683| 0.4613815468166347|\n|  4| 0.6684543991585701|0.19607867083102526|\n|  5|0.24378101415107944|0.20983377818911464|\n|  6|   0.31725730968555| 0.8332982931605045|\n|  7| 0.2009218252543502| 0.7970753910519424|\n|  8| 0.5173637377779307| 0.9490535744792827|\n|  9| 0.9528301401955117|0.12016201807848204|\n+---+-------------------+-------------------+\n\n-0.0265085681099\n9.16666666667\n-0.342472913375\n1.0\n"
      },
      "dateCreated": "Dec 7, 2015 7:29:43 AM",
      "dateStarted": "Dec 7, 2015 6:28:38 PM",
      "dateFinished": "Dec 7, 2015 6:28:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 4. Cross tabulation (a.k.a. contingency table)\n\nCross Tabulation provides a table of the frequency distribution for a set of variables. Cross-tabulation is used to observe the statistical significance (or independence) of variables. Users will be able to cross-tabulate two columns of a DataFrame in order to obtain the counts of the different pairs that are observed in those columns.   \n\nFrom [Cross Tabulation][1]: If the proportions of individuals in the different columns vary significantly between rows (or vice versa), we say that there is a contingency between the two variables. In other words, the two variables are not independent. If there is no contingency, we say that the two variables are independent.\n\n[1]:https://en.wikipedia.org/wiki/Contingency_table  ",
      "dateUpdated": "Dec 7, 2015 9:51:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436861395230_-1463783985",
      "id": "20150714-010955_1064537508",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003e4. Cross tabulation (a.k.a. contingency table)\u003c/h3\u003e\n\u003cp\u003eCross Tabulation provides a table of the frequency distribution for a set of variables. Cross-tabulation is used to observe the statistical significance (or independence) of variables. Users will be able to cross-tabulate two columns of a DataFrame in order to obtain the counts of the different pairs that are observed in those columns.\u003c/p\u003e\n\u003cp\u003eFrom \u003ca href\u003d\"https://en.wikipedia.org/wiki/Contingency_table\"\u003eCross Tabulation\u003c/a\u003e: If the proportions of individuals in the different columns vary significantly between rows (or vice versa), we say that there is a contingency between the two variables. In other words, the two variables are not independent. If there is no contingency, we say that the two variables are independent.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 14, 2015 1:09:55 AM",
      "dateStarted": "Dec 7, 2015 9:51:15 PM",
      "dateFinished": "Dec 7, 2015 9:51:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nnames \u003d [\"Alice\", \"Bob\", \"Mike\"]\nitems \u003d [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\nplaces \u003d [\"New York\", \"Beijing\"]\n\ndf1 \u003d sqlContext.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])\ndf1.show(10)\n\ndf1.stat.crosstab(\"name\", \"item\").show()\n",
      "dateUpdated": "Dec 8, 2015 12:26:01 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449515339405_395462766",
      "id": "20151207-190859_1023251810",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-------+\n| name|   item|\n+-----+-------+\n|Alice|   milk|\n|  Bob|  bread|\n| Mike| butter|\n|Alice| apples|\n|  Bob|oranges|\n| Mike|   milk|\n|Alice|  bread|\n|  Bob| butter|\n| Mike| apples|\n|Alice|oranges|\n+-----+-------+\nonly showing top 10 rows\n\n+---------+------+-------+------+----+-----+\n|name_item|apples|oranges|butter|milk|bread|\n+---------+------+-------+------+----+-----+\n|      Bob|     6|      7|     7|   6|    7|\n|     Mike|     7|      6|     7|   7|    6|\n|    Alice|     7|      7|     6|   7|    7|\n+---------+------+-------+------+----+-----+\n\n"
      },
      "dateCreated": "Dec 7, 2015 7:08:59 PM",
      "dateStarted": "Dec 8, 2015 12:26:01 AM",
      "dateFinished": "Dec 8, 2015 12:26:06 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 5. Frequent items",
      "dateUpdated": "Dec 7, 2015 7:30:58 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436861391331_1265640036",
      "id": "20150714-010951_955011640",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003col\u003e\n\u003cli\u003eFrequent items\u003c/li\u003e\n\u003c/ol\u003e\n"
      },
      "dateCreated": "Jul 14, 2015 1:09:51 AM",
      "dateStarted": "Jul 14, 2015 1:20:54 AM",
      "dateFinished": "Jul 14, 2015 1:20:54 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nx \u003d [(1, 2, 3), (6, 5, 6), (1, 5, 3), (1, 2, 3)]\ndf \u003d sqlContext.createDataFrame(x, [\"a\", \"b\", \"c\"])\ndf.show()\nfreq \u003d df.stat.freqItems([\"a\", \"b\", \"c\"], 0.5)  # why no output?\nfreq.collect()[0]       # why no output?\nprint(freq.collect()[0])\n\n# I don\u0027t know why \u00276\u0027 made it at 0.5",
      "dateUpdated": "Dec 8, 2015 6:09:23 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449536797569_-1051578068",
      "id": "20151208-010637_59909883",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+---+---+\n|  a|  b|  c|\n+---+---+---+\n|  1|  2|  3|\n|  6|  5|  6|\n|  1|  5|  3|\n|  1|  2|  3|\n+---+---+---+\n\nRow(a_freqItems\u003d[1, 6], b_freqItems\u003d[2, 5], c_freqItems\u003d[3, 6])\n"
      },
      "dateCreated": "Dec 8, 2015 1:06:37 AM",
      "dateStarted": "Dec 8, 2015 6:09:23 AM",
      "dateFinished": "Dec 8, 2015 6:09:23 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### 6. Mathematical functions\nWe can use `select()` to create columns we need. ",
      "dateUpdated": "Dec 8, 2015 5:58:22 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436860450800_724754770",
      "id": "20150714-005410_673965086",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003e6. Mathematical functions\u003c/h3\u003e\n\u003cp\u003eWe can use \u003ccode\u003eselect()\u003c/code\u003e to create columns we need.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 14, 2015 12:54:10 AM",
      "dateStarted": "Dec 8, 2015 5:58:22 AM",
      "dateFinished": "Dec 8, 2015 5:58:22 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import *  \ndf \u003d sqlContext.range(0, 10).withColumn(\u0027uniform\u0027, rand(seed\u003d10) * 3.14)    # if we add a show() at end of this line, the next one will not work\n\ndf.select(\u0027uniform\u0027, \n          toDegrees(\u0027uniform\u0027),\n          (pow(cos(df[\u0027uniform\u0027]), 2) + pow(sin(df.uniform), 2)).alias(\"cos^2 + sin^2\") # different syntax for sine and cosine\n          ).show()",
      "dateUpdated": "Dec 8, 2015 10:48:29 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436861804193_-43842193",
      "id": "20150714-011644_2121648684",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+------------------+------------------+------------------+\n|           uniform|  DEGREES(uniform)|     cos^2 + sin^2|\n+------------------+------------------+------------------+\n|  2.26864307689818| 129.9836735278388|               1.0|\n| 1.039974628945282| 59.58615703924848|               1.0|\n|0.7656847273071136|43.870503312322946|               1.0|\n|1.5307825562008464| 87.70737982255623|               1.0|\n|  2.09894681335791|120.26079382784157|0.9999999999999999|\n|0.7654723844343895| 43.85833696190617|               1.0|\n| 0.996187952412627| 57.07736527502282|               1.0|\n|0.6308945312986597| 36.14759396129742|               1.0|\n|1.6245221366227023| 93.07826215405575|               1.0|\n| 2.991886640213907|171.42247726583267|               1.0|\n+------------------+------------------+------------------+\n\n"
      },
      "dateCreated": "Jul 14, 2015 1:16:44 AM",
      "dateStarted": "Dec 8, 2015 10:48:29 AM",
      "dateFinished": "Dec 8, 2015 10:48:29 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1449554526173_57688980",
      "id": "20151208-060206_976634683",
      "dateCreated": "Dec 8, 2015 6:02:06 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "DF-statistic",
  "id": "2ATYEAYVV",
  "angularObjects": {
    "2B458XSTQ": [],
    "2B5XRQDUN": [],
    "2B497ZDYT": [],
    "2B5HA4K5Z": [],
    "2B7RWS7R2": [],
    "2B6MY65WY": [],
    "2B72GQPKA": [],
    "2B5KNPTJR": [],
    "2B83SG64T": [],
    "2B56DQK71": [],
    "2B79XCMHY": [],
    "2B6BSSFX7": [],
    "2B7NZSVBV": [],
    "2B62TSAA6": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}